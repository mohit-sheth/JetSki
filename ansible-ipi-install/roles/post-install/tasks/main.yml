- name: Wait 7m for all the nodes to settle
  command: sleep 900

- block:
  - name: Label infra nodes
    shell: |
       oc label node '{{ infra_node_string_1 }}' 'node-role.kubernetes.io/infra=' --overwrite 

  - name: Label infra nodes
    shell: |
       oc label node '{{ infra_node_string_2 }}' 'node-role.kubernetes.io/infra=' --overwrite

  - name: Label workload nodes
    shell: |
       oc label node '{{ workload_node_string }}' 'node-role.kubernetes.io/workload=' --overwrite
  when: infra_node_string_1 != "" or infra_node_string_2 != "" or workload_node_string != ""

- name: OVN block
  block:
  - name: Scale CVO Down for OVNKubernetes image patching
    command: oc scale -n openshift-cluster-version deployment.apps/cluster-version-operator --replicas=0
  
  - name: Patch new OVNKubernetes Image
    command: oc -n openshift-network-operator set env deployment.apps/network-operator OVN_IMAGE={{openshift_ovn_image}} RELEASE_VERSION="5.0.0"
 
  - name: Get ovnkube-node pod
    shell: oc get pods -n {{ ovn_namespace }} | tail -n1 | awk '{print $1}'
    register: ovnkube_node_pod


  - name: Check one ovnkube-node pod to see if ovn image has been updated
    shell: oc get pod -n {{ ovn_namespace }} {{ovnkube_node_pod.stdout}} -o json| jq '.spec.containers[] | select(.name=="ovnkube-node").image'
    register: ovnkube_node_image
    until: (ovnkube_node_image.stdout) == (openshift_ovn_image) or (ovnkube_node_image.stdout) == ""
    delay: 1
    retries: 600
  
  - name: Wait for OVNK new node images to roll out
    command: oc rollout status -n {{ ovn_namespace }} ds/ovnkube-node
  
  - name: Wait for OVNK new master images to roll out
    command: oc rollout status -n {{ ovn_namespace }} ds/ovnkube-master
  when: openshift_toggle_ovn_patch|bool

- name: Relabel the infra nodes
  shell: |
    oc label nodes --overwrite -l 'node-role.kubernetes.io/infra=' node-role.kubernetes.io/worker-
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: infra_node_string_1 != "" or infra_node_string_2 != ""

- name: Relabel the workload node
  shell: |
    oc label nodes --overwrite -l 'node-role.kubernetes.io/workload=' node-role.kubernetes.io/worker-
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: workload_node_string != ""

- name: Add additional label to worker nodes to provide ablity to isolate workloads on workers
  shell: |
    oc label nodes --overwrite -l 'node-role.kubernetes.io/worker=' computenode=true
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"

- name: Taint the workload node
  shell: |
    oc adm taint node -l node-role.kubernetes.io/workload= role=workload:NoSchedule --overwrite=true
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: workload_node_string != ""

- block:
  - name: Copy new cluster-monitoring-config
    template:
      src: cluster-monitoring-config.yml.j2
      dest: "{{ ansible_user_dir }}/cluster-monitoring-config.yml"
    register: copy_yaml
  
  - set_fact:
      cluster_monitoring_yaml_path: "{{ copy_yaml.dest }}"

  - name: Replace the cluster-monitoring-config ConfigMap
    shell: |
      oc create -f {{ cluster_monitoring_yaml_path }}
    ignore_errors: yes
    environment:
      KUBECONFIG: "{{ kubeconfig_path }}"
  when: infra_node_string_1 != "" or infra_node_string_2 != ""

- name: Apply new nodeSelector to infra workload components
  shell: |
    oc patch {{item.object}} {{item.type|default('',True)}} -n {{item.namespace}} -p {{item.patch}}
  with_items:
    - namespace: openshift-ingress-operator
      object: ingresscontrollers/default
      patch: |
        '{"spec": {"nodePlacement": {"nodeSelector": {"matchLabels": {"node-role.kubernetes.io/infra": ""}}}}}'
      type: "--type merge"

  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: infra_node_string_1 != "" or infra_node_string_2 != ""


- name: Deploy dittybopper
  include_tasks: 70_deploy_dittybopper.yml
  when: dittybopper_enable
